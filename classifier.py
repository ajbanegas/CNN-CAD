# -*- coding: utf-8 -*-
"""Cancer Explanation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZwrRExKg9J3uWRX93Ed1aw65B0oHcFh3
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, \
  PrecisionRecallDisplay, average_precision_score, precision_recall_curve
import seaborn as sns
from itertools import cycle
import os
import time

#BASEPATH = "datasets/clasif-cancer/"
#BASEPATH = "datasets/clasif-diabetes/"
BASEPATH = "datasets/clasif-malaria/"

def show_images(generator, rows = 1):
  x, _ = generator.next()
  fig, ax = plt.subplots(nrows = rows, ncols = 2)
  for i in range(2):
      image = x[i]
      ax[i].set_xticks([])
      ax[i].set_yticks([])
      ax.flatten()[i].imshow(image)

  plt.tight_layout()
  plt.savefig("images/generator.png")
  plt.close()


train_datagen = ImageDataGenerator(
      samplewise_center = True,  # set each sample mean to 0
      rotation_range = 180,  # randomly rotate images in the range (degrees, 0 to 180)
      zoom_range = .05, #0  # Randomly zoom image
      #width_shift_range = .1, #0,  # randomly shift images horizontally (fraction of total width)
      #height_shift_range = .1, #0,  # randomly shift images vertically (fraction of total height)
      horizontal_flip = True,  # randomly flip images
      vertical_flip = True,
      rescale = 1./255,
      brightness_range = [0.1, 1.5]
)

valid_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
        f"{BASEPATH}train/",
        target_size = (224, 224),  # all images will be resized to 224x224
        batch_size = 10,
        class_mode = "categorical"
)

valid_generator = valid_datagen.flow_from_directory(
        f"{BASEPATH}valid/",
        target_size = (224, 224),
        batch_size = 10,
        class_mode = "categorical"
)

#show_images(train_generator) # display a few training images
show_images(valid_generator) # display a few validation images

class_names = {v: k for k, v in train_generator.class_indices.items()}
print(class_names)


# Plot the AUC of the model
def plot_auc(model_name, history):
  fig, axs = plt.subplots(1,1, figsize=(6, 5))
  axs.plot(history.history['auc'])
  axs.plot(history.history['val_auc'])
  plt.title(f'Model AUC - {model_name}')
  plt.ylabel('AUC')
  plt.xlabel('Epoch')
  plt.legend(['train', 'val'], loc='lower right')
  plt.tight_layout()
  plt.savefig(f"images/auc-{model_name}.png")
  plt.close()

# Plot the loss of the model
def plot_loss(model_name, history):
  fig, axs = plt.subplots(1,1, figsize=(6, 5))
  axs.plot(history.history['loss'][5:])
  axs.plot(history.history['val_loss'][5:])
  plt.title(f'Model loss - {model_name}')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['train', 'val'], loc='upper right')
  plt.tight_layout()
  plt.savefig(f"images/loss-{model_name}.png")
  plt.close()

# Plot the precision-recall curve
def plot_precision_recall(model_name, yts, ypr):
  precision = dict()
  recall = dict()
  average_precision = dict()
  for i in range(4):
    precision[i], recall[i], _ = precision_recall_curve(yts == i, ypr == i)
    average_precision[i] = average_precision_score(yts == i, ypr == i)

  colors = cycle(["navy", "turquoise", "darkorange", "cornflowerblue", "teal"])
  _, ax = plt.subplots(figsize=(7, 8))
  for i, color in zip(range(4), colors):
    display = PrecisionRecallDisplay(
          recall = recall[i],
          precision = precision[i],
          average_precision = average_precision[i],
    )
    display.plot(ax=ax, name=f"Precision-recall for class {i}", color=color)

  plt.title("Precision-Recall curves")
  plt.tight_layout()
  plt.savefig(f"images/precrecall-{model_name}.png")
  plt.close()

# Plot the confusion matrix of a model
def plot_confusion_matrix(model_name, ypr):
  aux = list(class_names.keys())
  cm = confusion_matrix(valid_generator.labels, ypr, labels=aux)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=aux)
  disp.plot()
  plt.title(model_name)
  plt.tight_layout()
  plt.savefig(f"images/cm-{model_name}.png")
  plt.close()

# https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
# https://github.com/animikhaich/AlexNet-Tensorflow/blob/main/AlexNet_Prototype_Model.ipynb
def model_alexnet():
  # Input Layer
  inputs = keras.Input(shape=(224, 224, 3), name="alexnet_input")
  # Layer 1 - Convolutions
  l1_g1 = keras.layers.Conv2D(filters=48, kernel_size=11, strides=4, padding="same")(inputs)
  l1_g1 = keras.layers.BatchNormalization()(l1_g1)
  l1_g1 = keras.layers.ReLU()(l1_g1)
  l1_g1 = keras.layers.MaxPooling2D(pool_size=3, strides=2)(l1_g1)
  l1_g2 = keras.layers.Conv2D(filters=48, kernel_size=11, strides=4, padding="same")(inputs)
  l1_g2 = keras.layers.BatchNormalization()(l1_g2)
  l1_g2 = keras.layers.ReLU()(l1_g2)
  l1_g2 = keras.layers.MaxPooling2D(pool_size=3, strides=2)(l1_g2)
  # Layer 2 - Convolutions
  l2_g1 = keras.layers.Conv2D(filters=128, kernel_size=5, strides=1, padding="same")(l1_g1)
  l2_g1 = keras.layers.BatchNormalization()(l2_g1)
  l2_g1 = keras.layers.ReLU()(l2_g1)
  l2_g1 = keras.layers.MaxPooling2D(pool_size=3, strides=2)(l2_g1)
  l2_g2 = keras.layers.Conv2D(filters=128, kernel_size=5, strides=1, padding="same")(l1_g2)
  l2_g2 = keras.layers.BatchNormalization()(l2_g2)
  l2_g2 = keras.layers.ReLU()(l2_g2)
  l2_g2 = keras.layers.MaxPooling2D(pool_size=3, strides=2)(l2_g2)
  # Layer 3 - Convolutions
  l3_concat = keras.layers.concatenate([l2_g1, l2_g2], axis=-1)
  l3_g1 = keras.layers.Conv2D(filters=192, kernel_size=3, strides=1, padding="same")(l3_concat)
  l3_g1 = keras.layers.ReLU()(l3_g1)
  l3_g2 = keras.layers.Conv2D(filters=192, kernel_size=3, strides=1, padding="same")(l3_concat)
  l3_g2 = keras.layers.ReLU()(l3_g2)
  # Layer 4 - Convolutions
  l4_g1 = keras.layers.Conv2D(filters=192, kernel_size=3, strides=1, padding="same")(l3_g1)
  l4_g1 = keras.layers.ReLU()(l4_g1)
  l4_g2 = keras.layers.Conv2D(filters=192, kernel_size=3, strides=1, padding="same")(l3_g2)
  l4_g2 = keras.layers.ReLU()(l4_g2)
  # Layer 5 - Convolutions
  l5_g1 = keras.layers.Conv2D(filters=128, kernel_size=3, strides=1, padding="same")(l4_g1)
  l5_g1 = keras.layers.ReLU()(l5_g1)
  l5_g1 = keras.layers.MaxPooling2D(pool_size=3, strides=2)(l5_g1)
  l5_g2 = keras.layers.Conv2D(filters=128, kernel_size=3, strides=1, padding="same")(l4_g2)
  l5_g2 = keras.layers.ReLU()(l5_g2)
  l5_g2 = keras.layers.MaxPooling2D(pool_size=3, strides=2)(l5_g2)
  # Layer 6 - Dense
  #l6_pre = keras.layers.concatenate([l5_g1, l5_g2], axis=-1)
  #l6_pre = keras.layers.Flatten()(l6_pre)
  #l6 = keras.layers.Dense(units=4096)(l6_pre)
  #l6 = keras.layers.ReLU()(l6)
  #l6 = keras.layers.Dropout(rate=0.5)(l6)
  # Layer 7 - Dense
  #l7 = keras.layers.Dense(units=4096)(l6)
  #l7 = keras.layers.ReLU()(l7)
  #l7 = keras.layers.Dropout(rate=0.5)(l7)
  # Layer 8 - Dense
  #l8 = keras.layers.Dense(units=2)(l7)
  #l8 = keras.layers.Softmax(dtype=tf.float32, name="alexnet_output")(l8)
  # Complete model
  #alexnet = keras.models.Model(inputs=inputs, outputs=l8)
  alexnet = keras.models.Model(inputs=inputs, outputs=l5_g2)
  return alexnet
  
def evaluate_model(model_name, model, M):
  metrics = {
    'Loss': np.zeros(M),
    'Accuracy': np.zeros(M) ,
    'AUC': np.zeros(M),
    'Precision': np.zeros(M),
    'Recall': np.zeros(M),
    'F1 score': np.zeros(M)
  }

  for i in range(M):
    score = model.evaluate(valid_generator, verbose=0)
    metrics['Loss'][i] = score[0]
    metrics['Accuracy'][i] = score[1]
    metrics['AUC'][i] = score[2]
    metrics['Precision'][i] = score[3]
    metrics['Recall'][i] = score[4]
    metrics['F1 score'][i] = np.mean(score[5])

  avg_metrics = [
      np.mean(metrics['Loss']),
      np.mean(metrics['Accuracy']),
      np.mean(metrics['AUC']),
      np.mean(metrics['Precision']),
      np.mean(metrics['Recall']),
      np.mean(metrics['F1 score'])
  ]
  return avg_metrics

def build_model(base_model, training = False):
  # ## Adding New Layers
  inputs = keras.Input(shape = (224, 224, 3))
  # Separately from setting trainable on the model, we set training to False
  x = base_model(inputs, training = training)
  x = keras.layers.GlobalAveragePooling2D()(x)
  # A Dense classifier with multiple units (multiclass classification)
  x = keras.layers.Flatten()(x)
  x = keras.layers.Dense(4096, activation='relu')(x)
  x = keras.layers.Dense(4096, activation='relu')(x) # 2048
  x = keras.layers.Dropout(.2)(x) # .15
  outputs = keras.layers.Dense(2, activation='softmax')(x)
  return keras.Model(inputs, outputs)

def compile_model(model):
  model.compile(
      run_eagerly = True,
      loss = keras.losses.CategoricalCrossentropy(from_logits = True),
      optimizer = keras.optimizers.Adam(learning_rate = 0.0001), # SGD .001
      metrics = [
          'accuracy',
          keras.metrics.AUC(name = 'auc'),
          keras.metrics.Precision(name = 'precision'),
          keras.metrics.Recall(name = 'recall'),
          keras.metrics.F1Score(name = 'f1score')
      ]
  )
  return model


# Common parameters
cnn_params = {
    'weights': 'imagenet',  # Load weights pre-trained on ImageNet.
    'input_shape': (224, 224, 3),
    'include_top': False
}
history_dict = {}
metrics_dict = {}
time_dict = {}
M_EVAL_REPEATS = 30
initial_epochs = 20
fine_tune_epochs = 10
model_dict = {
    'VGG16': keras.applications.VGG16(**cnn_params),
    'VGG19': keras.applications.VGG19(**cnn_params),
    'ResNet50': keras.applications.ResNet50V2(**cnn_params),
    'ResNet101': keras.applications.ResNet101(**cnn_params),
    'DenseNet': keras.applications.DenseNet201(**cnn_params),
    'InceptionV3': keras.applications.InceptionV3(**cnn_params),
    'Xception': keras.applications.Xception(**cnn_params),
    'MobileNet': keras.applications.MobileNet(**cnn_params),
    'EfficientNet': keras.applications.EfficientNetV2B3(**cnn_params),
    'AlexNet': model_alexnet()
}


# Train and evalute each model separately
for model_name in model_dict.keys():
  print('Training', model_name)

  best_model_path = f"best-model-{model_name}.keras"
  CALLBACKS = [
      tf.keras.callbacks.TerminateOnNaN(),
      tf.keras.callbacks.EarlyStopping(
          monitor = "loss",
          verbose = 1,
          patience = 30,
          mode = "min",
          restore_best_weights = False
      ),
      tf.keras.callbacks.ModelCheckpoint(
          best_model_path,
          monitor = "loss",
          mode = "min",
          save_best_only = True,
          verbose = 1
      )
  ]  

  # If a previous weights file exists, then remove it
  if os.path.exists(best_model_path):
      os.remove(best_model_path)

  # Transfer learning
  # Create the model and freeze the convolutional layers
  base_model = model_dict[model_name]
  base_model.trainable = False
  model = build_model(base_model)
  model = compile_model(model)
  model.summary()

  # Train the model and load the weights of the best epoch
  time1 = time.time()
  history_dict[model_name] = model.fit_generator(
      train_generator,
      epochs = initial_epochs,
      steps_per_epoch = len(train_generator),
      validation_data = valid_generator,
      validation_steps = len(valid_generator),
      callbacks = CALLBACKS
  )

  # Load the weights of the best epoch and save the model
  model.load_weights(best_model_path)

  # Fine tuning
  # Unfreeze the top layers
  base_model.trainable = True
  fine_tune_at = int(len(base_model.layers) * .7) # .5
  print(f'Freezing {fine_tune_at} layers of {model_name}')
  for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

  # Compile the model
  model = compile_model(model)
  model.summary()

  total_epochs = initial_epochs + fine_tune_epochs
  history2 = model.fit_generator(
      train_generator,
      epochs = total_epochs,
      initial_epoch = history_dict[model_name].epoch[-1],
      steps_per_epoch = len(train_generator),
      validation_data = valid_generator,
      validation_steps = len(valid_generator),
      callbacks = CALLBACKS
  )

  time2 = time.time()
  time_dict[model_name] = time2 - time1

  # Load the weights of the best epoch and save the model
  model.load_weights(best_model_path)
  model.save(f'models/{model_name}.keras')

  # Plot classification metrics
  ypr = model.predict(valid_generator)
  ypr = np.argmax(ypr, axis = 1)

  for k in history2.history.keys():
      history_dict[model_name].history[k] = np.concatenate((history_dict[model_name].history[k], history2.history[k]), axis=0)

  plot_auc(model_name, history_dict[model_name])
  plot_loss(model_name, history_dict[model_name])
  plot_confusion_matrix(model_name, ypr)
  plot_precision_recall(model_name, valid_generator.labels, ypr)

  # Classification report
  print(classification_report(valid_generator.labels, ypr, target_names=list(class_names.values())))

  # Evaluate the performance of the model.
  # We do it N times to compute the average value of each metric
  metrics_dict[model_name] = evaluate_model(model_name, model, M_EVAL_REPEATS)

  # Remove the temporary file containing the best file
  os.remove(best_model_path)

# Print the average metrics of every model
f = open("output.txt", "w")
for model_name in metrics_dict.keys():
  f.write(f"Model: {model_name}\n")
  f.write(f"Loss: {metrics_dict[model_name][0]}\n")
  f.write(f"Accuracy: {metrics_dict[model_name][1]}\n")
  f.write(f"AUC: {metrics_dict[model_name][2]}\n")
  f.write(f"Precision: {metrics_dict[model_name][3]}\n")
  f.write(f"Recall: {metrics_dict[model_name][4]}\n")
  f.write(f"F1 score: {metrics_dict[model_name][5]}\n")
  f.write(f"Time (secs): {time_dict[model_name]}\n")
  f.write("\n")
f.close()

